{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQ0Q7nYspDyc"
   },
   "source": [
    "#PART1  Many tricks in DeepLearning\n",
    "\n",
    "## 在本节笔记中会使用tensorflow 2.0搭建一个简单的多层感知机作为分类器，然后运用多种改进训练的方法比较性能。\n",
    "\n",
    "通过本笔记，你可以：\n",
    "1. 掌握tensorflow 2.0 的使用方法\n",
    "2. 拥有自己的模型\n",
    "3. 体会不同trick带来的作用\n",
    "\n",
    "[2.0版本的使用指南](https://tensorflow.google.cn/beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1047,
     "status": "ok",
     "timestamp": 1569333393740,
     "user": {
      "displayName": "皮卡丘",
      "photoUrl": "",
      "userId": "03495401700709671572"
     },
     "user_tz": -480
    },
    "id": "QDjDJPjOmZff",
    "outputId": "13748091-7338-4aaa-d457-da44a0f723a8"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # google Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTAceia_rP31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 6s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x226464f7cc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow  as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout,BatchNormalization,Activation\n",
    "from tensorflow.keras import Model,datasets\n",
    "import numpy as np\n",
    "#从kears自带数据集导入fashion_mnist，每张图片都是28*28，train数目6w test数目1w, labels=10\n",
    "fashion_mnist = datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() #第一次运行会下载数据\n",
    "train_images = train_images / 255.0 #像素归一化\n",
    "test_images = test_images / 255.0\n",
    "#看看长什么样\n",
    "plt.figure()\n",
    "plt.imshow(train_images[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dEm_rDj4cLi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 28, 28), (?,)), types: (tf.float64, tf.uint8)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们训练一个对28*28的图片进行10分类的基础分类器\n",
    "\n",
    "EPOCH = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(1000).batch(BATCH_SIZE) #((None, 28, 28), (None,)) 为什么是None?\n",
    "test  = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle(1000).batch(BATCH_SIZE)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJI3ngWC4ls0"
   },
   "outputs": [],
   "source": [
    "#定义我们的第一个模型\n",
    "\n",
    "class BaseClassifier(Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.flatten = Flatten(input_shape=[28,28]) #flatten不影响第一维度batch\n",
    "    self.d1 = Dense(units=128,activation='relu')\n",
    "    self.d2 = Dense(units=32, activation='relu')\n",
    "    self.d3 = Dense(units=10, activation='softmax')\n",
    "  def call(self, input):\n",
    "    x = self.flatten(input)\n",
    "    x = self.d1(x)\n",
    "    x = self.d2(x)\n",
    "    output = self.d3(x)\n",
    "    return output\n",
    "\n",
    "#base的结果： EPOCH=10: train_loss=0.23673877120018005, train_acc=91.17166137695312，test_loss=0.34537169337272644,test_acc=88.23999786376953"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R82CL8FB5FsO"
   },
   "source": [
    "*当选择模型后，先运行模型代码块加载模型，然后重新运行下面代码，修改该行：model*= xxx()即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "455J7ZLapk0k"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdvancedClassifier4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-203f7f93a44b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtest_acc\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseCategoricalAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test_acc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdvancedClassifier4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#记得修改这里的模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AdvancedClassifier4' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_fn= tf.keras.losses.SparseCategoricalCrossentropy() #定义损失。由于是离散分类。\n",
    "opt = tf.keras.optimizers.Adam() #Adam优化器\n",
    "#定义全局训练和测试上的metrics.这些指标在 epoch 上累积值，然后打印出整体结果。所以在每次迭代都要把当次结果传进去，在一次迭代结束将它清空\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_acc  = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_acc  = tf.keras.metrics.SparseCategoricalAccuracy(name='test_acc')\n",
    "\n",
    "model = AdvancedClassifier4()  #记得修改这里的模型\n",
    "\n",
    "@tf.function\n",
    "def train_1_batch(img, lbl):\n",
    "  with tf.GradientTape() as tape: #tape上只有预测-计算损失两步\n",
    "    pred = model(img)\n",
    "    loss = loss_fn(lbl, pred)\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  train_loss(loss)\n",
    "  train_acc(lbl, pred)\n",
    "\n",
    "@tf.function\n",
    "def test_1_batch(img, lbl):\n",
    "  pred = model(img)\n",
    "  loss = loss_fn(lbl, pred)\n",
    "  test_loss(loss)\n",
    "  test_acc(lbl,pred)\n",
    "\n",
    "\n",
    "template = \"EPOCH={}: train_loss={}, train_acc={}, test_loss={},test_acc={}\"\n",
    "\n",
    "#实际上这是一个mini-batch过程，结合了online和batch的优点\n",
    "for i in range(EPOCH):\n",
    "  for img,lbl in train: #train是有若干个((32,28,28),(32,))的list\n",
    "    train_1_batch(img,lbl)\n",
    "  for img,lbl in test:\n",
    "    test_1_batch(img,lbl)\n",
    "  print(template.format(i+1, train_loss.result(), train_acc.result()*100, test_loss.result(), test_acc.result()*100))\n",
    "  #因为metrics对每个epoch累计，所以需要在epoch结束后清空重来\n",
    "  train_loss.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  train_acc.reset_states()\n",
    "  test_acc.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVSm8XPIwykx"
   },
   "source": [
    "## dropout\n",
    "\n",
    "\n",
    "dropout是Regularization防止过拟合的一种方法，每次在训练的前传过程中使该层每个神经元以p的概率保留，以1-p的概率失活。这样就相当于每个epoch都在训练不同的函数，最终每个神经元的权值相当于不同函数的复合结果。\n",
    "\n",
    "然鹅在test时，由于不可复制训练时的失活过程，所以需要对输出结果加以改动。设每个神经元的输出是x，则它的期望输出$p*x+(1-p)*0=px$。即在test时保留完整网络结构但对每层输出整体上乘以概率p。\n",
    "\n",
    "实际使用时只需要调用api加一层dropout层即可。rate默认0.5并且works well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IzE-PSh5Xfx"
   },
   "outputs": [],
   "source": [
    "class AdvancedClassifier1(Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.flatten = Flatten(input_shape=[28,28]) #flatten不影响第一维度batch\n",
    "    self.d1 = Dense(units=128,activation='relu')\n",
    "    self.drop1 = Dropout(0.5)\n",
    "    self.d2 = Dense(units=32, activation='relu')\n",
    "    self.drop2 = Dropout(0.5)\n",
    "    self.d3 = Dense(units=10, activation='softmax')\n",
    "  def call(self, input):\n",
    "    x = self.flatten(input)\n",
    "    x = self.d1(x)\n",
    "    x = self.drop1(x)\n",
    "    x = self.d2(x)\n",
    "    x = self.drop2(x)\n",
    "    output = self.d3(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRzkEzC46fB_"
   },
   "source": [
    "## Weight Decay\n",
    "\n",
    "Regularization的一种方法。如果权重矩阵过于复杂，可能会出现过拟合的情况。因此，对权重矩阵实行l2正则化(or l1/l1-l2混合)。\n",
    "\n",
    "实际使用时在调用keras.layers定义每层时加上kernel_regulator即可。注意这是layer-wise的操作，而不是对所有层遍历一遍求个总的出来。当然bias也需要。不过在非dense层，比如卷积层、LSTM层里就没有bias的正则化了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3L3s9oCj8G_b"
   },
   "outputs": [],
   "source": [
    "class AdvancedClassifier2(Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.flatten = Flatten(input_shape=[28,28]) #flatten不影响第一维度batch\n",
    "    self.d1 = Dense(units=128,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "    self.drop1 = Dropout(0.5)\n",
    "    self.d2 = Dense(units=32, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "    self.drop2 = Dropout(0.5)\n",
    "    self.d3 = Dense(units=10, activation='softmax')\n",
    "  def call(self, input):\n",
    "    x = self.flatten(input)\n",
    "    x = self.d1(x)\n",
    "    x = self.drop1(x)\n",
    "    x = self.d2(x)\n",
    "    x = self.drop2(x)\n",
    "    output = self.d3(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRQX6hti_-kC"
   },
   "source": [
    "## Early Stopping\n",
    "Regularization的一种方法。增加一个验证集，如果验证集的loss曲线开始上升说明模型有过拟合的趋势，这样可以在该epoch训练结束后就中止训练。\n",
    "\n",
    "如果选择keras model.fit方法，该方法包含参数callbacks=, callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)(只是一个栗子)。monitor是监控值，可以拿train_acc/val_acc等，patience是若监控值保持基本不变多少个epoch后停止训练。这两个是最重要的参数，如果自己写方法的话（不用model.fit）也沿用这个思路。但是关于最佳权重矩阵，很可能earlystopping发现的时候已经过了最佳点。api可以帮你恢复最佳的，如果自己写的话...[一个思路](https://www.datalearner.com/blog/1051537860479157)\n",
    "\n",
    "演示时，从train_data中拿出来1w份作为valid_set，之后画train和valid的loss曲线。选择AdvancedClassifier2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3173,
     "status": "error",
     "timestamp": 1569327401519,
     "user": {
      "displayName": "皮卡丘",
      "photoUrl": "",
      "userId": "03495401700709671572"
     },
     "user_tz": -480
    },
    "id": "so4_QeRuCAO-",
    "outputId": "037f7fbe-1e0b-4a03-d6a8-a3d7a95d8ed6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow  as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout,BatchNormalization\n",
    "from tensorflow.keras import Model,datasets\n",
    "import numpy as np\n",
    "fashion_mnist = datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() #第一次运行会下载数据\n",
    "train_images = train_images / 255.0 \n",
    "test_images = test_images / 255.0\n",
    "\n",
    "EPOCH = 20\n",
    "BATCH_SIZE = 32\n",
    "valid_images = train_images[0:10000]\n",
    "valid_labels = train_labels[0:10000]\n",
    "train_images_ = train_images[10000:]\n",
    "train_labels_ = train_labels[10000:]\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((train_images_, train_labels_)).shuffle(1000).batch(BATCH_SIZE) \n",
    "valid = tf.data.Dataset.from_tensor_slices((valid_images, valid_labels)).shuffle(1000).batch(BATCH_SIZE)\n",
    "test  = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "class AdvancedClassifier3(Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.flatten = Flatten(input_shape=[28,28]) #flatten不影响第一维度batch\n",
    "    self.d1 = Dense(units=128,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "    self.drop1 = Dropout(0.5)\n",
    "    self.d2 = Dense(units=32, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "    self.drop2 = Dropout(0.5)\n",
    "    self.d3 = Dense(units=10, activation='softmax')\n",
    "  def call(self, input):\n",
    "    x = self.flatten(input)\n",
    "    x = self.d1(x)\n",
    "    x = self.drop1(x)\n",
    "    x = self.d2(x)\n",
    "    x = self.drop2(x)\n",
    "    output = self.d3(x)\n",
    "    return output\n",
    "\n",
    "loss_fn= tf.keras.losses.SparseCategoricalCrossentropy() #定义损失。由于是离散分类。\n",
    "opt = tf.keras.optimizers.Adam() #Adam优化器\n",
    "#定义全局训练和测试上的metrics.这些指标在 epoch 上累积值，然后打印出整体结果。所以在每次迭代都要把当次结果传进去，在一次迭代结束将它清空\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_acc  = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "valid_acc  = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_acc')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_acc  = tf.keras.metrics.SparseCategoricalAccuracy(name='test_acc')\n",
    "\n",
    "model = AdvancedClassifier2()  \n",
    "\n",
    "train_loss = list()\n",
    "valid_loss = list()\n",
    "\n",
    "@tf.function\n",
    "def train_1_batch(img, lbl):\n",
    "  with tf.GradientTape() as tape: #tape上只有预测-计算损失两步\n",
    "    pred = model(img)\n",
    "    loss = loss_fn(lbl, pred)\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  train_loss(loss)\n",
    "  train_acc(lbl, pred)\n",
    "\n",
    "@tf.function\n",
    "def test_1_batch(img, lbl):\n",
    "  pred = model(img)\n",
    "  loss = loss_fn(lbl, pred)\n",
    "  test_loss(loss)\n",
    "  test_acc(lbl,pred)\n",
    "\n",
    "@tf.function\n",
    "def valid_1_batch(img, lbl):\n",
    "  pred = model(img)\n",
    "  loss = loss_fn(lbl, pred)\n",
    "  valid_loss(loss)\n",
    "  valid_acc(lbl,pred)\n",
    "\n",
    "template = \"EPOCH={}: train_loss={}, train_acc={}, valid_loss={}, valid_acc={}, test_loss={},test_acc={}\"\n",
    "\n",
    "#实际上这是一个mini-batch过程，结合了online和batch的优点\n",
    "for i in range(EPOCH):\n",
    "  for img,lbl in train: \n",
    "    train_1_batch(img,lbl)\n",
    "  for img,lbl in valid:\n",
    "    valid_1_batch(img,lbl)\n",
    "  for img,lbl in test:\n",
    "    test_1_batch(img,lbl)\n",
    "  \n",
    "  print(template.format(i+1, train_loss.result(), train_acc.result()*100, valid_loss.result(), valid_acc.result()*100, test_loss.result(), test_acc.result()*100))\n",
    "  train_loss.append(train_loss.result())\n",
    "  valid_loss.append(valid_loss.result())\n",
    "  #因为metrics对每个epoch累计，所以需要在epoch结束后清空重来\n",
    "  train_loss.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  train_acc.reset_states()\n",
    "  test_acc.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKsHsqrUFSc6"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "epoch = np.arange(1,21)\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.plot(epoch, train_loss,color='red')\n",
    "plt.plot(epoch, valid_loss,color='grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzFKLnaAPP6h"
   },
   "source": [
    "##BatchNormalization\n",
    "\n",
    "[参考资料](https://zhuanlan.zhihu.com/p/24810318)\n",
    "BN解决的问题是激励层对输入值范围的敏感度。比如sigmoid那样的激励，许多输入值最后都被压缩到两端。所以为了使每层的输出在经过激励前分布更合理，就引入了BN。注意BN层除了改变了原始数据分布外，还有一个反Normalization的过程$y\\leftarrow \\gamma x_{i}^{'}+\\beta$，两个参数$\\gamma \\beta$需要网络自适应学习。这部分是当归一化起到反作用时抵消副作用的效果。最坏的情况，就是把数据分布还原到最开始：$\\gamma=\\sqrt{\\sigma_{B}^{2}+\\epsilon}$ , $\\beta=\\mu_{B} $。\n",
    "\n",
    "实际使用时，首先是没有激励的网络层，之后调用api增加BN层，最后加激励层即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TG1xOmxIfqng"
   },
   "outputs": [],
   "source": [
    "class AdvancedClassifier4(Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.flatten = Flatten(input_shape=[28,28]) #flatten不影响第一维度batch\n",
    "    self.d1 = Dense(units=128,kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "    self.bn1 = BatchNormalization()\n",
    "    self.act1 = Activation('relu')\n",
    "    self.drop1 = Dropout(0.5)\n",
    "    self.d2 = Dense(units=32,kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "    self.bn2 = BatchNormalization()\n",
    "    self.act2 = Activation('relu')\n",
    "    self.drop2 = Dropout(0.5)\n",
    "    self.d3 = Dense(units=10, activation='softmax')\n",
    "  def call(self, input):\n",
    "    x = self.flatten(input)\n",
    "    x = self.d1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.act1(x)\n",
    "    x = self.drop1(x)\n",
    "    x = self.d2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.act2(x)\n",
    "    x = self.drop2(x)\n",
    "    output = self.d3(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jOFpmUv0zd5P"
   },
   "source": [
    "## Momentum\n",
    "\n",
    "\"一定程度上保留前几次的更新方向，并用当前batch的方向微调。\"\n",
    "$d_\\theta^{t}= d_\\theta loss(f(x),y)+\\beta d_\\theta^{t-1}$\n",
    "\n",
    "$\\theta^{t}\\leftarrow \\theta^{t}+\\eta d_\\theta^{t}  $\n",
    "\n",
    "动量法本质上是一种指数加权平均，为了消除单个batch产生的梯度过大的影响，所以要对一段时间内的度量进行平均，使其朝着最优点前进.[参考资料1](https://www.jiqizhixin.com/graph/technologies/d6ee5e5b-43ff-4c41-87ff-f34c234d0e32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKHj0U8D82h3"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(nesterov=True) #开启动量选项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rVSm8XPIwykx",
    "aRzkEzC46fB_",
    "dRQX6hti_-kC",
    "bzFKLnaAPP6h"
   ],
   "name": "大数据分析L2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
