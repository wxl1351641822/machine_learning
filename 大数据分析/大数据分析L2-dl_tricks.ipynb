{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oQ0Q7nYspDyc"},"source":"#PART1  Many tricks in DeepLearning\n\ncopyright shiboshen19s@ict.ac.cn\n## 在本节笔记中会使用tensorflow 2.0搭建一个简单的多层感知机作为分类器，然后运用多种改进训练的方法比较性能。\n\n通过本笔记，你可以：\n1. 掌握tensorflow 2.0 的使用方法\n2. 拥有自己的模型\n3. 体会不同trick带来的作用\n\n[2.0版本的使用指南](https://tensorflow.google.cn/beta)"},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":1047,"status":"ok","timestamp":1569333393740,"user":{"displayName":"皮卡丘","photoUrl":"","userId":"03495401700709671572"},"user_tz":-480},"id":"QDjDJPjOmZff","outputId":"13748091-7338-4aaa-d457-da44a0f723a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow 2.x selected.\n"]}],"source":["try:\n","  # google Colab only\n","  %tensorflow_version 2.x\n","except Exception:\n","    pass"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"dTAceia_rP31"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import tensorflow  as tf\n","from tensorflow.keras.layers import Flatten, Dense, Dropout,BatchNormalization,Activation\n","from tensorflow.keras import Model,datasets\n","import numpy as np\n","#从kears自带数据集导入fashion_mnist，每张图片都是28*28，train数目6w test数目1w, labels=10\n","fashion_mnist = datasets.fashion_mnist\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() #第一次运行会下载数据\n","train_images = train_images / 255.0 #像素归一化\n","test_images = test_images / 255.0\n","#看看长什么样\n","plt.figure()\n","plt.imshow(train_images[10])"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"1dEm_rDj4cLi"},"outputs":[],"source":["#我们训练一个对28*28的图片进行10分类的基础分类器\n","\n","EPOCH = 20\n","BATCH_SIZE = 32\n","\n","train = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(1000).batch(BATCH_SIZE) #((None, 28, 28), (None,)) 为什么是None?\n","test  = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle(1000).batch(BATCH_SIZE)\n","train"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"nJI3ngWC4ls0"},"outputs":[],"source":["#定义我们的第一个模型\n","\n","class BaseClassifier(Model):\n","  def __init__(self):\n","    super().__init__()\n","    self.flatten = Flatten(input_shape=[28,28]) #flatten不影响第一维度batch\n","    self.d1 = Dense(units=128,activation='relu')\n","    self.d2 = Dense(units=32, activation='relu')\n","    self.d3 = Dense(units=10, activation='softmax')\n","  def call(self, input):\n","    x = self.flatten(input)\n","    x = self.d1(x)\n","    x = self.d2(x)\n","    output = self.d3(x)\n","    return output\n","\n","#base的结果： EPOCH=10: train_loss=0.23673877120018005, train_acc=91.17166137695312，test_loss=0.34537169337272644,test_acc=88.23999786376953"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R82CL8FB5FsO"},"source":["*当选择模型后，先运行模型代码块加载模型，然后重新运行下面代码，修改该行：model*= xxx()即可"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"455J7ZLapk0k"},"outputs":[],"source":["\n","loss_fn= tf.keras.losses.SparseCategoricalCrossentropy() #定义损失。由于是离散分类。\n","opt = tf.keras.optimizers.Adam() #Adam优化器\n","#定义全局训练和测试上的metrics.这些指标在 epoch 上累积值，然后打印出整体结果。所以在每次迭代都要把当次结果传进去，在一次迭代结束将它清空\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_acc  = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_acc  = tf.keras.metrics.SparseCategoricalAccuracy(name='test_acc')\n","\n","model = AdvancedClassifier4()  #记得修改这里的模型\n","\n","@tf.function\n","def train_1_batch(img, lbl):\n","  with tf.GradientTape() as tape: #tape上只有预测-计算损失两步\n","    pred = model(img)\n","    loss = loss_fn(lbl, pred)\n","  grads = tape.gradient(loss, model.trainable_variables)\n","  opt.apply_gradients(zip(grads, model.trainable_variables))\n","  train_loss(loss)\n","  train_acc(lbl, pred)\n","\n","@tf.function\n","def test_1_batch(img, lbl):\n","  pred = model(img)\n","  loss = loss_fn(lbl, pred)\n","  test_loss(loss)\n","  test_acc(lbl,pred)\n","\n","\n","template = \"EPOCH={}: train_loss={}, train_acc={}, test_loss={},test_acc={}\"\n","\n","#实际上这是一个mini-batch过程，结合了online和batch的优点\n","for i in range(EPOCH):\n","  for img,lbl in train: #train是有若干个((32,28,28),(32,))的list\n","    train_1_batch(img,lbl)\n","  for img,lbl in test:\n","    test_1_batch(img,lbl)\n","  print(template.format(i+1, train_loss.result(), train_acc.result()*100, test_loss.result(), test_acc.result()*100))\n","  #因为metrics对每个epoch累计，所以需要在epoch结束后清空重来\n","  train_loss.reset_states()\n","  test_loss.reset_states()\n","  train_acc.reset_states()\n","  test_acc.reset_states()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rVSm8XPIwykx"},"source":["## dropout\n","\n","\n","dropout是Regularization防止过拟合的一种方法，每次在训练的前传过程中使该层每个神经元以p的概率保留，以1-p的概率失活。这样就相当于每个epoch都在训练不同的函数，最终每个神经元的权值相当于不同函数的复合结果。\n","\n","然鹅在test时，由于不可复制训练时的失活过程，所以需要对输出结果加以改动。设每个神经元的输出是x，则它的期望输出$p*x+(1-p)*0=px$。即在test时保留完整网络结构但对每层输出整体上乘以概率p。\n","\n","实际使用时只需要调用api加一层dropout层即可。rate默认0.5并且works well!"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"1IzE-PSh5Xfx"},"outputs":[],"source":["class AdvancedClassifier1(Model):\n","  def __init__(self):\n","    super().__init__()\n","    self.flatten = Flatten(input_shape=[28,28]) #flatten不影响第一维度batch\n","    self.d1 = Dense(units=128,activation='relu')\n","    self.drop1 = Dropout(0.5)\n","    self.d2 = Dense(units=32, activation='relu')\n","    self.drop2 = Dropout(0.5)\n","    self.d3 = Dense(units=10, activation='softmax')\n","  def call(self, input):\n","    x = self.flatten(input)\n","    x = self.d1(x)\n","    x = self.drop1(x)\n","    x = self.d2(x)\n","    x = self.drop2(x)\n","    output = self.d3(x)\n","    return output"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aRzkEzC46fB_"},"source":["## Weight Decay\n","\n","Regularization的一种方法。如果权重矩阵过于复杂，可能会出现过拟合的情况。因此，对权重矩阵实行l2正则化(or l1/l1-l2混合)。\n","\n","实际使用时在调用keras.layers定义每层时加上kernel_regulator即可。注意这是layer-wise的操作，而不是对所有层遍历一遍求个总的出来。当然bias也需要。不过在非dense层，比如卷积层、LSTM层里就没有bias的正则化了。"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"3L3s9oCj8G_b"},"outputs":[],"source":["class AdvancedClassifier2(Model):\n","  def __init__(self):\n","    super().__init__()\n","    self.flatten = Flatten(input_shape=[28,28]) #flatten不影响第一维度batch\n","    self.d1 = Dense(units=128,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n","    self.drop1 = Dropout(0.5)\n","    self.d2 = Dense(units=32, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n","    self.drop2 = Dropout(0.5)\n","    self.d3 = Dense(units=10, activation='softmax')\n","  def call(self, input):\n","    x = self.flatten(input)\n","    x = self.d1(x)\n","    x = self.drop1(x)\n","    x = self.d2(x)\n","    x = self.drop2(x)\n","    output = self.d3(x)\n","    return output"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dRQX6hti_-kC"},"source":["## Early Stopping\n","Regularization的一种方法。增加一个验证集，如果验证集的loss曲线开始上升说明模型有过拟合的趋势，这样可以在该epoch训练结束后就中止训练。\n","\n","如果选择keras model.fit方法，该方法包含参数callbacks=, callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)(只是一个栗子)。monitor是监控值，可以拿train_acc/val_acc等，patience是若监控值保持基本不变多少个epoch后停止训练。这两个是最重要的参数，如果自己写方法的话（不用model.fit）也沿用这个思路。但是关于最佳权重矩阵，很可能earlystopping发现的时候已经过了最佳点。api可以帮你恢复最佳的，如果自己写的话...[一个思路](https://www.datalearner.com/blog/1051537860479157)\n","\n","演示时，从train_data中拿出来1w份作为valid_set，之后画train和valid的loss曲线。选择AdvancedClassifier2。"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":549},"colab_type":"code","executionInfo":{"elapsed":3173,"status":"error","timestamp":1569327401519,"user":{"displayName":"皮卡丘","photoUrl":"","userId":"03495401700709671572"},"user_tz":-480},"id":"so4_QeRuCAO-","outputId":"037f7fbe-1e0b-4a03-d6a8-a3d7a95d8ed6"},"outputs":[{"name":"stdout","output_type":"stream","text":["(50000, 28, 28)\n","WARNING:tensorflow:Entity [] could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: unknown callable type \"<class 'list'>\"\n","WARNING: Entity [] could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: unknown callable type \"<class 'list'>\"\n"]},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-7e1b181cd94f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlbl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtrain_1_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlbl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mvalid_1_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0minitializer_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    355\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    356\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 357\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   1543\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1545\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   1546\u001b[0m         self._function_attributes)\n\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    713\u001b[0m                                           converted_func)\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-2-7e1b181cd94f>:63 train_1_batch *\n        train_loss(loss)\n\n    TypeError: 'list' object is not callable\n"]}],"source":["import matplotlib.pyplot as plt\n","import tensorflow  as tf\n","from tensorflow.keras.layers import Flatten, Dense, Dropout,BatchNormalization\n","from tensorflow.keras import Model,datasets\n","import numpy as np\n","fashion_mnist = datasets.fashion_mnist\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() #第一次运行会下载数据\n","train_images = train_images / 255.0 \n","test_images = test_images / 255.0\n","\n","EPOCH = 20\n","BATCH_SIZE = 32\n","valid_images = train_images[0:10000]\n","valid_labels = train_labels[0:10000]\n","train_images_ = train_images[10000:]\n","train_labels_ = train_labels[10000:]\n","\n","train = tf.data.Dataset.from_tensor_slices((train_images_, train_labels_)).shuffle(1000).batch(BATCH_SIZE) \n","valid = tf.data.Dataset.from_tensor_slices((valid_images, valid_labels)).shuffle(1000).batch(BATCH_SIZE)\n","test  = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle(1000).batch(BATCH_SIZE)\n","\n","class AdvancedClassifier3(Model):\n","  def __init__(self):\n","    super().__init__()\n","    self.flatten = Flatten(input_shape=[28,28]) #flatten不影响第一维度batch\n","    self.d1 = Dense(units=128,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n","    self.drop1 = Dropout(0.5)\n","    self.d2 = Dense(units=32, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n","    self.drop2 = Dropout(0.5)\n","    self.d3 = Dense(units=10, activation='softmax')\n","  def call(self, input):\n","    x = self.flatten(input)\n","    x = self.d1(x)\n","    x = self.drop1(x)\n","    x = self.d2(x)\n","    x = self.drop2(x)\n","    output = self.d3(x)\n","    return output\n","\n","loss_fn= tf.keras.losses.SparseCategoricalCrossentropy() #定义损失。由于是离散分类。\n","opt = tf.keras.optimizers.Adam() #Adam优化器\n","#定义全局训练和测试上的metrics.这些指标在 epoch 上累积值，然后打印出整体结果。所以在每次迭代都要把当次结果传进去，在一次迭代结束将它清空\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_acc  = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n","valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n","valid_acc  = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_acc')\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_acc  = tf.keras.metrics.SparseCategoricalAccuracy(name='test_acc')\n","\n","model = AdvancedClassifier2()  \n","\n","train_loss = list()\n","valid_loss = list()\n","\n","@tf.function\n","def train_1_batch(img, lbl):\n","  with tf.GradientTape() as tape: #tape上只有预测-计算损失两步\n","    pred = model(img)\n","    loss = loss_fn(lbl, pred)\n","  grads = tape.gradient(loss, model.trainable_variables)\n","  opt.apply_gradients(zip(grads, model.trainable_variables))\n","  train_loss(loss)\n","  train_acc(lbl, pred)\n","\n","@tf.function\n","def test_1_batch(img, lbl):\n","  pred = model(img)\n","  loss = loss_fn(lbl, pred)\n","  test_loss(loss)\n","  test_acc(lbl,pred)\n","\n","@tf.function\n","def valid_1_batch(img, lbl):\n","  pred = model(img)\n","  loss = loss_fn(lbl, pred)\n","  valid_loss(loss)\n","  valid_acc(lbl,pred)\n","\n","template = \"EPOCH={}: train_loss={}, train_acc={}, valid_loss={}, valid_acc={}, test_loss={},test_acc={}\"\n","\n","#实际上这是一个mini-batch过程，结合了online和batch的优点\n","for i in range(EPOCH):\n","  for img,lbl in train: \n","    train_1_batch(img,lbl)\n","  for img,lbl in valid:\n","    valid_1_batch(img,lbl)\n","  for img,lbl in test:\n","    test_1_batch(img,lbl)\n","  \n","  print(template.format(i+1, train_loss.result(), train_acc.result()*100, valid_loss.result(), valid_acc.result()*100, test_loss.result(), test_acc.result()*100))\n","  train_loss.append(train_loss.result())\n","  valid_loss.append(valid_loss.result())\n","  #因为metrics对每个epoch累计，所以需要在epoch结束后清空重来\n","  train_loss.reset_states()\n","  test_loss.reset_states()\n","  train_acc.reset_states()\n","  test_acc.reset_states()\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"QKsHsqrUFSc6"},"outputs":[],"source":["%matplotlib inline\n","epoch = np.arange(1,21)\n","plt.figure(figsize=(8,10))\n","plt.plot(epoch, train_loss,color='red')\n","plt.plot(epoch, valid_loss,color='grey')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bzFKLnaAPP6h"},"source":["##BatchNormalization\n","\n","[参考资料](https://zhuanlan.zhihu.com/p/24810318)\n","BN解决的问题是激励层对输入值范围的敏感度。比如sigmoid那样的激励，许多输入值最后都被压缩到两端。所以为了使每层的输出在经过激励前分布更合理，就引入了BN。注意BN层除了改变了原始数据分布外，还有一个反Normalization的过程$y\\leftarrow \\gamma x_{i}^{'}+\\beta$，两个参数$\\gamma \\beta$需要网络自适应学习。这部分是当归一化起到反作用时抵消副作用的效果。最坏的情况，就是把数据分布还原到最开始：$\\gamma=\\sqrt{\\sigma_{B}^{2}+\\epsilon}$ , $\\beta=\\mu_{B} $。\n","\n","实际使用时，首先是没有激励的网络层，之后调用api增加BN层，最后加激励层即可。"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"TG1xOmxIfqng"},"outputs":[],"source":["class AdvancedClassifier4(Model):\n","  def __init__(self):\n","    super().__init__()\n","    self.flatten = Flatten(input_shape=[28,28]) #flatten不影响第一维度batch\n","    self.d1 = Dense(units=128,kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n","    self.bn1 = BatchNormalization()\n","    self.act1 = Activation('relu')\n","    self.drop1 = Dropout(0.5)\n","    self.d2 = Dense(units=32,kernel_regularizer=tf.keras.regularizers.l2(0.01), bias_regularizer=tf.keras.regularizers.l2(0.01))\n","    self.bn2 = BatchNormalization()\n","    self.act2 = Activation('relu')\n","    self.drop2 = Dropout(0.5)\n","    self.d3 = Dense(units=10, activation='softmax')\n","  def call(self, input):\n","    x = self.flatten(input)\n","    x = self.d1(x)\n","    x = self.bn1(x)\n","    x = self.act1(x)\n","    x = self.drop1(x)\n","    x = self.d2(x)\n","    x = self.bn2(x)\n","    x = self.act2(x)\n","    x = self.drop2(x)\n","    output = self.d3(x)\n","    return output"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jOFpmUv0zd5P"},"source":["## Momentum\n","\n","\"一定程度上保留前几次的更新方向，并用当前batch的方向微调。\"\n","$d_\\theta^{t}= d_\\theta loss(f(x),y)+\\beta d_\\theta^{t-1}$\n","\n","$\\theta^{t}\\leftarrow \\theta^{t}+\\eta d_\\theta^{t}  $\n","\n","动量法本质上是一种指数加权平均，为了消除单个batch产生的梯度过大的影响，所以要对一段时间内的度量进行平均，使其朝着最优点前进.[参考资料1](https://www.jiqizhixin.com/graph/technologies/d6ee5e5b-43ff-4c41-87ff-f34c234d0e32)\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"eKHj0U8D82h3"},"outputs":[],"source":["opt = tf.keras.optimizers.SGD(nesterov=True) #开启动量选项"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}